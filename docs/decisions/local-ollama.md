# Local Ollama

Ollama is available as a [docker image](https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image)
but they are explicit that to get good GPU performance you should run the standalone
application as docker in MacOS does NOT support GPUs.
